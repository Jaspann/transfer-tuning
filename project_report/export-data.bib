
@misc{chen_tvm_2018,
	title = {{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning},
	url = {http://arxiv.org/abs/1802.04799},
	doi = {10.48550/arXiv.1802.04799},
	shorttitle = {{TVM}},
	abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class {GPUs}. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., {FPGAs}, {ASICs}) -- requires significant manual effort. We propose {TVM}, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. {TVM} solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that {TVM} delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power {CPU}, mobile {GPU}, and server-class {GPUs}. We also demonstrate {TVM}'s ability to target new accelerator back-ends, such as the {FPGA}-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
	number = {{arXiv}:1802.04799},
	publisher = {{arXiv}},
	author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	urldate = {2024-12-16},
	date = {2018-10-05},
	eprinttype = {arxiv},
	eprint = {1802.04799},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{chetlur_cudnn_2014,
	title = {{cuDNN}: Efficient Primitives for Deep Learning},
	url = {http://arxiv.org/abs/1410.0759},
	doi = {10.48550/arXiv.1410.0759},
	shorttitle = {{cuDNN}},
	abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the {HPC} community by libraries such as the Basic Linear Algebra Subroutines ({BLAS}). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to {BLAS}, with optimized routines for deep learning workloads. Our implementation contains routines for {GPUs}, although similarly to the {BLAS} library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating {cuDNN} into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	number = {{arXiv}:1410.0759},
	publisher = {{arXiv}},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	urldate = {2024-12-15},
	date = {2014-12-18},
	eprinttype = {arxiv},
	eprint = {1410.0759},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
}

@misc{canesche_explore_2024,
	title = {Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning Kernel Schedulers with Coordinate Descent},
	url = {http://arxiv.org/abs/2406.20037},
	doi = {10.48550/arXiv.2406.20037},
	shorttitle = {Explore as a Storm, Exploit as a Raindrop},
	abstract = {Machine-learning models consist of kernels, which are algorithms applying operations on tensors -- data indexed by a linear combination of natural numbers. Examples of kernels include convolutions, transpositions, and vectorial products. There are many ways to implement a kernel. These implementations form the kernel's optimization space. Kernel scheduling is the problem of finding the best implementation, given an objective function -- typically execution speed. Kernel optimizers such as Ansor, Halide, and {AutoTVM} solve this problem via search heuristics, which combine two phases: exploration and exploitation. The first step evaluates many different kernel optimization spaces. The latter tries to improve the best implementations by investigating a kernel within the same space. For example, Ansor combines kernel generation through sketches for exploration and leverages an evolutionary algorithm to exploit the best sketches. In this work, we demonstrate the potential to reduce Ansor's search time while enhancing kernel quality by incorporating Droplet Search, an {AutoTVM} algorithm, into Ansor's exploration phase. The approach involves limiting the number of samples explored by Ansor, selecting the best, and exploiting it with a coordinate descent algorithm. By applying this approach to the first 300 kernels that Ansor generates, we usually obtain better kernels in less time than if we let Ansor analyze 10,000 kernels. This result has been replicated in 20 well-known deep-learning models ({AlexNet}, {ResNet}, {VGG}, {DenseNet}, etc.) running on four architectures: an {AMD} Ryzen 7 (x86), an {NVIDIA} A100 tensor core, an {NVIDIA} {RTX} 3080 {GPU}, and an {ARM} A64FX. A patch with this combined approach was approved in Ansor in February 2024. As evidence of the generality of this search methodology, a similar patch, achieving equally good results, was submitted to {TVM}'s {MetaSchedule} in June 2024.},
	number = {{arXiv}:2406.20037},
	publisher = {{arXiv}},
	author = {Canesche, Michael and Verma, Gaurav and Pereira, Fernando Magno Quintao},
	urldate = {2024-12-15},
	date = {2024-07-15},
	eprinttype = {arxiv},
	eprint = {2406.20037},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2024-12-15},
	date = {2012},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	url = {https://ieeexplore.ieee.org/document/7298594},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC}14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC}14 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1--9},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2024-12-15},
	date = {2015-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Computer architecture, Computer vision, Convolutional codes, Neural networks, Object detection, Sparse matrices, Visualization},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	number = {{arXiv}:1512.03385},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2024-12-15},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2024-12-15},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{canesche_droplet_2024,
	title = {The Droplet Search Algorithm for Kernel Scheduling},
	volume = {21},
	issn = {1544-3566},
	url = {https://dl.acm.org/doi/10.1145/3650109},
	doi = {10.1145/3650109},
	abstract = {Kernel scheduling is the problem of finding the most efficient implementation for a computational kernel. Identifying this implementation involves experimenting with the parameters of compiler optimizations, such as the size of tiling windows and unrolling factors. This article shows that it is possible to organize these parameters as points in a coordinate space. The function that maps these points to the running time of kernels, in general, will not determine a convex surface. However, this article provides empirical evidence that the origin of this surface (an unoptimized kernel) and its global optimum (the fastest kernel) reside on a convex region. We call this hypothesis the “droplet expectation.” Consequently, a search method based on the Coordinate Descent algorithm tends to find the optimal kernel configuration quickly if the hypothesis holds. This approach—called Droplet Search—has been available in Apache {TVM} since April of 2023. Experimental results with six large deep learning models on various computing devices ({ARM}, Intel, {AMD}, and {NVIDIA}) indicate that Droplet Search is not only as effective as other {AutoTVM} search techniques but also 2 to 10 times faster. Moreover, models generated by Droplet Search are competitive with those produced by {TVM}’s {AutoScheduler} (Ansor), despite the latter using 4 to 5 times more code transformations than {AutoTVM}.},
	pages = {35:1--35:28},
	number = {2},
	journaltitle = {{ACM} Trans. Archit. Code Optim.},
	author = {Canesche, Michael and Rosário, Vanderson and Borin, Edson and Quintão Pereira, Fernando},
	urldate = {2024-12-04},
	date = {2024-05-21},
}

@misc{zheng_ansor_2023,
	title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
	url = {http://arxiv.org/abs/2006.06762},
	doi = {10.48550/arXiv.2006.06762},
	shorttitle = {Ansor},
	abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy. We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel {CPU}, {ARM} {CPU}, and {NVIDIA} {GPU} by up to \$3.8{\textbackslash}times\$, \$2.6{\textbackslash}times\$, and \$1.7{\textbackslash}times\$, respectively.},
	number = {{arXiv}:2006.06762},
	publisher = {{arXiv}},
	author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2024-12-04},
	date = {2023-10-15},
	eprinttype = {arxiv},
	eprint = {2006.06762},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance, Computer Science - Programming Languages, Statistics - Machine Learning},
}

@misc{chen_learning_2019,
	title = {Learning to Optimize Tensor Programs},
	url = {http://arxiv.org/abs/1805.08166},
	doi = {10.48550/arXiv.1805.08166},
	abstract = {We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as {cuDNN} where only a narrow range of server class {GPUs} are well-supported. The reliance on hardware-specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power {CPU}, mobile {GPU}, and server-class {GPU}.},
	number = {{arXiv}:1805.08166},
	publisher = {{arXiv}},
	author = {Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	urldate = {2024-12-04},
	date = {2019-01-08},
	eprinttype = {arxiv},
	eprint = {1805.08166},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gibson_transfer-tuning_2022,
	title = {Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation},
	url = {http://arxiv.org/abs/2201.05587},
	doi = {10.48550/arXiv.2201.05587},
	shorttitle = {Transfer-Tuning},
	abstract = {Auto-scheduling for tensor programs is a process where a search algorithm automatically explores candidate schedules (program transformations) for a given program on a target hardware platform to improve its performance. However this can be a very time consuming process depending on the complexity of the tensor program and the capacity of the target device, with often many thousands of program variants being explored. To address this, in this paper we introduce the idea of transfer-tuning, a novel approach to identify and reuse auto-schedules between tensor programs. We demonstrate this concept using Deep Neural Networks ({DNNs}), taking sets of auto-schedules from pre-tuned {DNNs} and using them to reduce the inference time of a new {DNN}. We compare transfer-tuning against the state-of-the-art Ansor auto-scheduler, defining the maximum possible speedup for a given {DNN} model as what Ansor achieves using its recommended full tuning time. On a server-class {CPU} and across 11 widely used {DNN} models, we observe that transfer-tuning achieves up to \$88.41{\textbackslash}\%\$ (\$49.13{\textbackslash}\%\$ on average) of this maximum speedup, while Ansor requires \$6.5{\textbackslash}times\$ more search time on average to match it. We also evaluate transfer-tuning on a constrained edge {CPU} and observe that the differences in search time are exacerbated, with Ansor requiring \$10.8{\textbackslash}times\$ more time on average to match transfer-tuning's speedup, which further demonstrates its value. Our code is available at https://www.github.com/{gicLAB}/transfer-tuning},
	number = {{arXiv}:2201.05587},
	publisher = {{arXiv}},
	author = {Gibson, Perry and Cano, José},
	urldate = {2024-12-04},
	date = {2022-09-07},
	eprinttype = {arxiv},
	eprint = {2201.05587},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Performance, Computer Science - Programming Languages},
}

@misc{shao2022tnsrprgopt,
      title={Tensor Program Optimization with Probabilistic Programs}, 
      author={Junru Shao and Xiyou Zhou and Siyuan Feng and Bohan Hou and Ruihang Lai and Hongyi Jin and Wuwei Lin and Masahiro Masuda and Cody Hao Yu and Tianqi Chen},
      year={2022},
      eprint={2205.13603},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.13603}, 
}

@article{RaganKelley2013HalideAL,
  title={Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
  author={Jonathan Ragan-Kelley and Connelly Barnes and Andrew Adams and Sylvain Paris and Fr{\'e}do Durand and Saman P. Amarasinghe},
  journal={Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:5885207}
}

 @inproceedings{Jia_etal_taso_2019, address={Huntsville Ontario Canada}, title={TASO: optimizing deep learning computation with automatic generation of graph substitutions}, ISBN={978-1-4503-6873-5}, url={https://dl.acm.org/doi/10.1145/3341301.3359630}, DOI={10.1145/3341301.3359630}, booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles}, publisher={ACM}, author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex}, year={2019}, month=oct, pages={47–62}, language={en} }

 @article{apollo, title={Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization}, volume={4}, journal={Proceedings of Machine Learning and Systems}, author={Zhao, Jie and Gao, Xiong and Xia, Ruijie and Zhang, Zhaochuang and Chen, Deshi and Chen, Lei and Zhang, Renwei and Geng, Zhen and Cheng, Bin and Jin, Xuefeng}, year={2022}, month=apr, pages={1–19}, language={en} }

\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Optimizing AI Scheduling on GPUs with Ansor, Transfer Tuning, and Droplet Search}  

\author{\IEEEauthorblockN{1\textsuperscript{st} William Parker}
\IEEEauthorblockA{\textit{Charles W. Davidson College of Engineering} \\
\textit{San Jose State University}\\
San Jose, CA, USA \\
william.j.parker@sjsu.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Volodymyr Makarenko}
\IEEEauthorblockA{\textit{Charles W. Davidson College of Engineering} \\
\textit{San Jose State University}\\
San Jose, CA, USA \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Tarun Sanjeev Banala}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Charles W. Davidson College of Engineering}\\
San Jose, CA, USA \\
email address or ORCID}
}

\maketitle

\begin{abstract}
To interact with a GPU to run tasks such as AI models, the task need to be scheduled and 
coordinated with the rest of the device through the task scheduler.
Libraries have been made which have hand tuned the kernel to run these models, 
but this tuning only exists for the set of GPUs the library supports and may not optimize novel operations.
To solve this, work has been done to automatically search for optimizations for the kernel for any hardware and model combination. 
This work proposes combining two existing techniques, Droplet Search, and Transfer Tuning, on top of the Ansor task scheduler generator, 
to improve the speed at which the search finds optimizations. 
We find that combining these techniques can lead to improvements compared to when Transfer Tuning is used by itself with Ansor by up to 10\%.
The source code is available at https://github.com/Jaspann/transfer-tuning.
\end{abstract}

\begin{IEEEkeywords}
Task Schedulers, Artificial Intelligence, Deep Learning, GPUs.
\end{IEEEkeywords}

\section{Introduction}
Individuals, academics, and industry all run different AI models on their devices.
AI has become a part of many digital systems, and very in capabilities, such as 
natural language processing, image generation, object detection, facial recognition, and much more.
Each model has different requirements that it must devote resources to, 
and each hardware component that the model may run on has different specifications.
Optimizations that work on one GPU may not work on another, 
and due to the large verity of GPU models and options in building deep learning models, 
this creates a near impossible task to optimize every combination.
This is especially noticeable for consumer grade technology, where there is a large verity in hardware
while end user applications are quickly pushing AI small models to run locally rather then deal with hosting the model centrally.

Traditional solutions like cuDNN propose libraries that 
optimize the most important parts of the AI models on the most popular GPUs.
This paradigm does not work for everyone though, 
especially considering the rapid pace of AI development and deployment paired with growing historical and new options for compute.
Developers using these models suffer as well, as they need to sacrifice options that in theory are a better fit for the system,
but due to a lack of support cannot be implemented unless there is significant development put into these problems 
that are completely separate from the development of the system.

In response, techniques such as Ansor from the TVM (Tensor Virtual Machine) project have been developed to 
programmatically search for optimization techniques to apply to optimize the model with the task scheduler.
Our research lyes in testing combining optimization techniques in Ansor to measure how they compare apart versus when combined together. 

\section{Background}


\section{Motivation}

\section{Design}
To implement our test, we decided to start development based on Transfer Tuning's work. 

\section{Evaluation}

\section{Case Studies}

\section{Related Works}

\section{Conclusion}


% Figure:

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}



% Use the `biblatex` package for citations. 
% Preferably generate the biblatex file via Zotero. 
% If that does not work with IEEE, use \bibitem as recommended

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \bibitem{b8} D. P. Kingma and M. Welling, ``Auto-encoding variational Bayes,'' 2013, arXiv:1312.6114. [Online]. Available: https://arxiv.org/abs/1312.6114
% \bibitem{b9} S. Liu, ``Wi-Fi Energy Detection Testbed (12MTC),'' 2023, gitHub repository. [Online]. Available: https://github.com/liustone99/Wi-Fi-Energy-Detection-Testbed-12MTC
% \bibitem{b10} ``Treatment episode data set: discharges (TEDS-D): concatenated, 2006 to 2009.'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, August, 2013, DOI:10.3886/ICPSR30122.v2
% \bibitem{b11} K. Eves and J. Valasek, ``Adaptive control for singularly perturbed systems examples,'' Code Ocean, Aug. 2023. [Online]. Available: https://codeocean.com/capsule/4989235/tree
% \end{thebibliography}

\end{document}
